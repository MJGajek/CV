!pip install -U git+https://github.com/openai/whisper.git

import whisper
import logging
import os
import json
import subprocess

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Define the path to your audio file
audio_path = "/content/audio.mp3"  # Adjust as needed
kaldi_output_dir = "/content/kaldi_diarization_output"

# Ensure directories exist
os.makedirs(kaldi_output_dir, exist_ok=True)

# Step 1: Transcription with Whisper
def transcribe_audio(file_path):
    """Transcribes the audio file using Whisper, including timestamps."""
    logging.info("Starting transcription using Whisper for file: %s", file_path)
    try:
        model = whisper.load_model("base")  # Choose model size as needed
        result = model.transcribe(file_path)
        logging.info("Transcription completed successfully.")

        # Return segments with timestamps
        segments = [
            {
                "start": segment["start"],
                "end": segment["end"],
                "text": segment["text"]
            }
            for segment in result["segments"]
        ]
        return segments
    except Exception as e:
        logging.error("Failed to transcribe audio: %s", e)
        return None

# Step 2: Diarization with Kaldi
def perform_speaker_diarization(audio_path):
    """Performs speaker diarization using Kaldi."""
    logging.info("Starting speaker diarization on audio file: %s", audio_path)
    try:
        # Run Kaldi diarization script (assumes Kaldi is installed and properly configured)
        kaldi_cmd = f"your_kaldi_diarization_script.sh {audio_path} {kaldi_output_dir}"
        subprocess.run(kaldi_cmd, shell=True, check=True)

        # Read and parse RTTM output file generated by Kaldi
        rttm_path = os.path.join(kaldi_output_dir, "output.rttm")  # Path to RTTM output
        with open(rttm_path, "r") as rttm_file:
            diarization = []
            for line in rttm_file:
                parts = line.strip().split()
                start_time = float(parts[3])
                duration = float(parts[4])
                speaker = parts[7]
                diarization.append({
                    "start": start_time,
                    "end": start_time + duration,
                    "speaker": speaker
                })
        
        logging.info("Speaker diarization completed successfully.")
        return diarization
    except Exception as e:
        logging.error("Failed to perform speaker diarization: %s", e)
        return None

# Step 3: Combine Transcription and Diarization
def combine_transcription_diarization(transcript_segments, diarization_segments):
    """Aligns transcription with diarization by matching timestamps."""
    aligned_transcript = []
    for segment in transcript_segments:
        start = segment["start"]
        end = segment["end"]
        text = segment["text"]

        # Find the corresponding speaker label from diarization
        speaker_label = "Unknown"
        for diar in diarization_segments:
            if start >= diar["start"] and end <= diar["end"]:
                speaker_label = diar["speaker"]
                break

        aligned_transcript.append({
            "start": start,
            "end": end,
            "text": text,
            "speaker": speaker_label
        })
    return aligned_transcript

# Step 4: Save Transcription with Speaker Labels
def save_transcript(transcript):
    """Saves the transcribed text to a JSON file."""
    try:
        with open("transcript_with_speakers.json", "w", encoding="utf-8") as file:
            json.dump(transcript, file, ensure_ascii=False, indent=2)
        logging.info("Transcription saved to file: transcript_with_speakers.json")
        return True
    except Exception as e:
        logging.error("Failed to save transcription: %s", e)
        return False

# RUN
# Step 1: Transcribe the audio
transcript_segments = transcribe_audio(audio_path)
if transcript_segments:
    # Step 2: Perform speaker diarization
    diarization_segments = perform_speaker_diarization(audio_path)
    if diarization_segments:
        # Step 3: Combine transcription and diarization data
        aligned_transcript = combine_transcription_diarization(transcript_segments, diarization_segments)

        # Step 4: Save the aligned transcript with speaker labels
        success = save_transcript(aligned_transcript)
        if success:
            logging.info("Aligned subtitle transcription with speaker labels saved successfully.")
        else:
            logging.error("Failed to save aligned subtitles.")
    else:
        logging.warning("Speaker diarization was not successful.")
else:
    logging.error("Transcription failed.")
